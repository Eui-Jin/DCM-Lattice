{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99acc0-d9c9-41e6-81d6-aaca16769173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Python basics\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.core.pylabtools import figsize\n",
    "from time import time\n",
    "from scipy.stats import norm, uniform\n",
    "import math\n",
    "import collections\n",
    "import collections.abc\n",
    "from collections import OrderedDict    # For recording the model specification \n",
    "collections.Iterable = collections.abc.Iterable\n",
    "import pylogit as pl  \n",
    "import lxml\n",
    "\n",
    "## DNN\n",
    "import tensorflow as tf\n",
    "import tensorflow_lattice as tfl\n",
    "from scikeras.wrappers import KerasRegressor, KerasClassifier\n",
    "from tensorflow.keras.layers import Activation,Input, Dense, Reshape, Concatenate, Layer, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, Flatten,LeakyReLU,ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from statsmodels.formula.api import logit\n",
    "import statsmodels.api as sm\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.distributions.log_normal as log_normal\n",
    "import torch.distributions.bernoulli as bernoulli \n",
    "\n",
    "\n",
    "## PDP\n",
    "from sklearn.inspection import PartialDependenceDisplay, partial_dependence\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error,accuracy_score,mean_squared_error\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "\n",
    "# Get the prediction metrics\n",
    "def brier_multi(targets, probs):\n",
    "     return np.mean(np.sum((probs - targets)**2, axis=1))\n",
    "def save_clipboard(X):\n",
    "    pd.DataFrame(X).to_clipboard(index=False,header=False)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def compute_VOT_POP(X,Y):\n",
    "    VOT_temp = []\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(Y.shape[0]):\n",
    "            VOT_temp.append(X[i]/Y[j])\n",
    "    return np.array(VOT_temp)\n",
    "\n",
    "def compute_VOT_IND(X,Y):\n",
    "    VOT_temp = []\n",
    "    for i in range(X.shape[0]):\n",
    "        #VOT_temp.append(compute_VOT_POP(X[i,:],Y[i,:]).mean())\n",
    "         VOT_temp.append(np.median(compute_VOT_POP(X[i,:],Y[i,:])))\n",
    "    return np.array(VOT_temp)\n",
    "\n",
    "import warnings\n",
    "\n",
    "#suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478042b-b069-43e9-a917-d0aec32b0a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import tensorflow as tf\n",
    "\n",
    "# gpu = tf.config.experimental.get_visible_devices('GPU')[0]\n",
    "# tf.config.experimental.set_memory_growth(device = gpu, enable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7327b00-bdc2-4851-8c10-6a71fcc47971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_wide = pd.read_csv(\"data/data_METRO.csv\")\n",
    "df_wide = df_wide[(df_wide.AGE != 6) & (df_wide.CHOICE != 0) & (df_wide.INCOME != 4) & (df_wide.PURPOSE != 9) & (df_wide.CAR_AV != 0)]\n",
    "\n",
    "# Create AGE dummy\n",
    "df_wide[\"AGES\"] = 0\n",
    "df_wide.loc[df_wide[\"AGE\"] == 1,\"AGES\"] = 1\n",
    "df_wide.loc[df_wide[\"AGE\"] == 2,\"AGES\"] = 2\n",
    "df_wide.loc[df_wide[\"AGE\"] == 3,\"AGES\"] = 3\n",
    "df_wide.loc[df_wide[\"AGE\"] == 4,\"AGES\"] = 4\n",
    "df_wide.loc[df_wide[\"AGE\"] == 5,\"AGES\"] = 5\n",
    "\n",
    "# Create INCOME dummy\n",
    "df_wide[\"INCOMES\"] = 0\n",
    "df_wide.loc[df_wide[\"INCOME\"].isin([0,1]),\"INCOMES\"] = 1\n",
    "df_wide.loc[df_wide[\"INCOME\"] == 2,\"INCOMES\"] = 2\n",
    "df_wide.loc[df_wide[\"INCOME\"] == 3,\"INCOMES\"] = 3\n",
    "df_wide.loc[df_wide[\"INCOME\"] == 4,\"INCOMES\"] = 4\n",
    "\n",
    "\n",
    "# Create PURPOSE dummy\n",
    "df_wide[\"PURPOSES\"] = 0\n",
    "df_wide.loc[df_wide[\"PURPOSE\"].isin([1,5]),\"PURPOSES\"] = 1\n",
    "df_wide.loc[df_wide[\"PURPOSE\"].isin([2,6]),\"PURPOSES\"] = 2\n",
    "df_wide.loc[df_wide[\"PURPOSE\"].isin([3,7]),\"PURPOSES\"] = 3\n",
    "df_wide.loc[df_wide[\"PURPOSE\"].isin([4,8]),\"PURPOSES\"] = 4\n",
    "\n",
    "\n",
    "\n",
    "# Scale the travel time column by 60 to convert raw units (minutes) to hours\n",
    "df_wide[\"TRAIN_TT\"] = df_wide[\"TRAIN_TT\"] / 100.0\n",
    "df_wide[\"SM_TT\"] = df_wide[\"SM_TT\"] / 100.0\n",
    "df_wide[\"CAR_TT\"] = df_wide[\"CAR_TT\"] / 100.0\n",
    "\n",
    "# Scale the headway column by 60 to convert raw units (minutes) to hours\n",
    "df_wide[\"TRAIN_HE\"] = df_wide[\"TRAIN_HE\"] / 100.0\n",
    "df_wide[\"SM_HE\"] = df_wide[\"SM_HE\"] / 100.0\n",
    "\n",
    "df_wide[\"TRAIN_TC\"] = df_wide[\"TRAIN_CO\"] / 100.0\n",
    "df_wide[\"SM_TC\"] = df_wide[\"SM_CO\"] / 100.0\n",
    "df_wide[\"CAR_TC\"] = df_wide[\"CAR_CO\"] / 100.0\n",
    "\n",
    "df_wide.loc[(df_wide[\"GA\"] == 1),\"TRAIN_TC\"] = 0\n",
    "df_wide.loc[(df_wide[\"GA\"] == 1),\"SM_TC\"] = 0\n",
    "\n",
    "\n",
    "##########\n",
    "# Create various dummy variables to describe the choice context of a given\n",
    "# invidual for each choice task.\n",
    "##########\n",
    "\n",
    "# Create AGE dummy\n",
    "#df_wide[\"AGE_1\"] = (df_wide[\"AGE\"] == 1).astype(int)\n",
    "df_wide[\"AGE_2\"] = (df_wide[\"AGE\"] == 2).astype(int)\n",
    "df_wide[\"AGE_3\"] = (df_wide[\"AGE\"] == 3).astype(int)\n",
    "df_wide[\"AGE_4\"] = (df_wide[\"AGE\"] == 4).astype(int)\n",
    "df_wide[\"AGE_5\"] = (df_wide[\"AGE\"] == 5).astype(int)\n",
    "\n",
    "# Create INCOME dummy\n",
    "#df_wide[\"INCOME_01\"] = (df_wide[\"INCOME\"].isin([0,1])).astype(int)\n",
    "df_wide[\"INCOME_2\"] = (df_wide[\"INCOME\"] == 2).astype(int)\n",
    "df_wide[\"INCOME_3\"] = (df_wide[\"INCOME\"] == 3).astype(int)\n",
    "df_wide[\"INCOME_4\"] = (df_wide[\"INCOME\"] == 4).astype(int)\n",
    "\n",
    "# Create PURPOSE dummy\n",
    "#df_wide[\"PURPOSE_1\"] = (df_wide[\"PURPOSE\"].isin([1,5])).astype(int)\n",
    "df_wide[\"PURPOSE_2\"] = (df_wide[\"PURPOSE\"].isin([2,6])).astype(int)\n",
    "df_wide[\"PURPOSE_3\"] = (df_wide[\"PURPOSE\"].isin([3,7])).astype(int)\n",
    "df_wide[\"PURPOSE_4\"] = (df_wide[\"PURPOSE\"].isin([4,8])).astype(int)\n",
    "\n",
    "# Create LUGGAGE dummy\n",
    "#df_wide[\"LUGGAGE_0\"] = (df_wide[\"LUGGAGE\"] == 0).astype(int)\n",
    "df_wide[\"LUGGAGE_1\"] = (df_wide[\"LUGGAGE\"] == 1).astype(int)\n",
    "df_wide[\"LUGGAGE_3\"] = (df_wide[\"LUGGAGE\"] == 3).astype(int)\n",
    "\n",
    "# Create a dummy variable indicating that a person is NOT first class\n",
    "df_wide[\"TRAIN_CLASS\"] = 1 - df_wide[\"FIRST\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4b46a-bcd9-422a-b0f5-408c5f2e485c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_wide['TRAIN_TT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b675b4e-0c72-4688-b26f-ad913097e386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the list of individual specific variables\n",
    "ind_variables = ['ID','GA','AGE_2', 'AGE_3', 'AGE_4',\n",
    "       'AGE_5', 'INCOME_2', 'INCOME_3', 'INCOME_4', 'PURPOSE_2', 'PURPOSE_3',\n",
    "       'PURPOSE_4','AGES', 'INCOMES','PURPOSES']\n",
    "\n",
    "\n",
    "# Specify the variables that vary across individuals and some or all alternatives\n",
    "# The keys are the column names that will be used in the long format dataframe.\n",
    "# The values are dictionaries whose key-value pairs are the alternative id and\n",
    "# the column name of the corresponding column that encodes that variable for\n",
    "# the given alternative. Examples below.\n",
    "alt_varying_variables = {u'TT': dict([(1, 'TRAIN_TT'),\n",
    "                                               (2, 'SM_TT'),\n",
    "                                               (3, 'CAR_TT')]),\n",
    "                          u'TC': dict([(1, 'TRAIN_TC'),\n",
    "                                                (2, 'SM_TC'),\n",
    "                                                (3, 'CAR_TC')]),\n",
    "                          u'HE': dict([(1, 'TRAIN_HE'),\n",
    "                                            (2, 'SM_HE')]),\n",
    "                          u'SEAT': dict([(2, \"SM_SEATS\")])}\n",
    "\n",
    "# Specify the availability variables\n",
    "# Note that the keys of the dictionary are the alternative id's.\n",
    "# The values are the columns denoting the availability for the\n",
    "# given mode in the dataset.\n",
    "availability_variables = {1: 'TRAIN_AV',\n",
    "                          2: 'SM_AV', \n",
    "                          3: 'CAR_AV'}\n",
    "\n",
    "##########\n",
    "# Determine the columns for: alternative ids, the observation ids and the choice\n",
    "##########\n",
    "# The 'custom_alt_id' is the name of a column to be created in the long-format data\n",
    "# It will identify the alternative associated with each row.\n",
    "custom_alt_id = \"mode_id\"\n",
    "\n",
    "# Create a custom id column that ignores the fact that this is a \n",
    "# panel/repeated-observations dataset. Note the +1 ensures the id's start at one.\n",
    "obs_id_column = \"custom_id\"\n",
    "df_wide[obs_id_column] = np.arange(df_wide.shape[0], dtype=int) + 1\n",
    "\n",
    "# Create a variable recording the choice column\n",
    "choice_column = \"CHOICE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9c5fc-e0f0-4dfb-96f5-023647b1e57c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform the conversion to long-format\n",
    "df_long = pl.convert_wide_to_long(df_wide, \n",
    "                                           ind_variables, \n",
    "                                           alt_varying_variables, \n",
    "                                           availability_variables, \n",
    "                                           obs_id_column, \n",
    "                                           choice_column,\n",
    "                                           new_alt_id_name=custom_alt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab46faa-def7-42f1-92c5-18e087936fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_long[\"TTxAGE_2\"] = df_long[\"TT\"]*df_long[\"AGE_2\"]\n",
    "df_long[\"TTxAGE_3\"] = df_long[\"TT\"]*df_long[\"AGE_3\"]\n",
    "df_long[\"TTxAGE_4\"] = df_long[\"TT\"]*df_long[\"AGE_4\"]\n",
    "df_long[\"TTxAGE_5\"] = df_long[\"TT\"]*df_long[\"AGE_5\"]\n",
    "\n",
    "df_long[\"TTxINCOME_2\"] = df_long[\"TT\"]*df_long[\"INCOME_2\"]\n",
    "df_long[\"TTxINCOME_3\"] = df_long[\"TT\"]*df_long[\"INCOME_3\"]\n",
    "\n",
    "df_long[\"TTxPURPOSE_2\"] = df_long[\"TT\"]*df_long[\"PURPOSE_2\"]\n",
    "df_long[\"TTxPURPOSE_3\"] = df_long[\"TT\"]*df_long[\"PURPOSE_3\"]\n",
    "df_long[\"TTxPURPOSE_4\"] = df_long[\"TT\"]*df_long[\"PURPOSE_4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b63b0c-5bff-482e-a25d-beeddd05bbd2",
   "metadata": {},
   "source": [
    "## 1. MNL-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c5ecd-865e-4dda-a9d4-b0e5bee59be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternative-specific Utility Specification\n",
    "\n",
    "basic_specification = OrderedDict()\n",
    "basic_names = OrderedDict()\n",
    "\n",
    "basic_specification[\"intercept\"] = [1, 2]\n",
    "basic_names[\"intercept\"] = ['ASC Train',\n",
    "                            'ASC Swissmetro']\n",
    "\n",
    "basic_specification[\"TT\"] = [1, 2, 3]\n",
    "basic_names[\"TT\"] = ['Travel Time(Train)',\n",
    "                     'Travel Time(Swissmetro)',\n",
    "                     'Travel Time(Car)']\n",
    "\n",
    "basic_specification[\"TC\"] = [[1, 2, 3]]\n",
    "basic_names[\"TC\"] = ['Travel Cost(All)']\n",
    "\n",
    "basic_specification[\"HE\"] = [1, 2]\n",
    "basic_names[\"HE\"] = [\"Headway(Train)\",\n",
    "                     \"Headway(Swissmetro)\"]\n",
    "\n",
    "basic_specification[\"SEAT\"] = [2]\n",
    "basic_names[\"SEAT\"] = ['Airline Seat(Swissmetro)']\n",
    "\n",
    "basic_specification[\"GA\"] = [1,2]\n",
    "basic_names[\"GA\"] = ['GA(Train)',\n",
    "                     'GA(Swissmetro)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194cbaba-56b1-44c0-ae53-8243ff0da000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Original: 1005\n",
    "\n",
    "SS = GroupShuffleSplit(n_splits=2, train_size=0.7, random_state=1234)\n",
    "train_idx, rem_idx = next(SS.split(df_wide,groups = df_wide['custom_id']))\n",
    "\n",
    "df_wide_train = df_wide.iloc[train_idx]\n",
    "df_wide_rem = df_wide.iloc[rem_idx]\n",
    "\n",
    "SS2 = GroupShuffleSplit(n_splits=2, train_size=0.5, random_state=1234)\n",
    "test_idx, valid_idx = next(SS2.split(df_wide_rem,groups = df_wide_rem['custom_id']))\n",
    "df_wide_test = df_wide_rem.iloc[test_idx]\n",
    "df_wide_valid = df_wide_rem.iloc[valid_idx]\n",
    "\n",
    "df_train = df_long[df_long.custom_id.isin(df_wide_train['custom_id'].unique())]\n",
    "df_test = df_long[df_long.custom_id.isin(df_wide_test['custom_id'].unique())]\n",
    "\n",
    "\n",
    "\n",
    "# Estimate the multinomial logit model (MNL)\n",
    "MNL_A = pl.create_choice_model(data=df_train,\n",
    "                                alt_id_col=custom_alt_id,\n",
    "                                obs_id_col=obs_id_column,\n",
    "                                choice_col=choice_column,\n",
    "                                specification=basic_specification,\n",
    "                                model_type=\"MNL\",\n",
    "                                names=basic_names)\n",
    "\n",
    "# Specify the initial values and method for the optimization.\n",
    "MNL_A.fit_mle(np.zeros(11),maxiter=3000)\n",
    "\n",
    "\n",
    "# Get the prediction metrics\n",
    "predict_train = np.argmax(MNL_A.predict(df_train).reshape(-1,3),axis=1)\n",
    "predict_test =  np.argmax(MNL_A.predict(df_test).reshape(-1,3),axis=1)\n",
    "\n",
    "y_train = np.array(df_train.CHOICE).reshape(-1,3)\n",
    "y_test = np.array(df_test.CHOICE).reshape(-1,3)\n",
    "y_train_cat = np.argmax(np.array(df_train.CHOICE).reshape(-1,3),axis=1)\n",
    "y_test_cat = np.argmax(np.array(df_test.CHOICE).reshape(-1,3),axis=1)\n",
    "\n",
    "\n",
    "train_p = MNL_A.predict(df_train).reshape(-1,3)\n",
    "test_p = MNL_A.predict(df_test).reshape(-1,3)\n",
    "\n",
    "test_NLL = round((F.cross_entropy(input=torch.log(tensor(test_p)),target=torch.Tensor(y_test)).numpy()).item(),3)\n",
    "train_NLL =  round((F.cross_entropy(input=torch.log(tensor(train_p)),target=torch.Tensor(y_train)).numpy()).item(),3)\n",
    "\n",
    "\n",
    "train_acc = round(accuracy_score(y_train_cat,predict_train),3)\n",
    "test_acc = round(accuracy_score(y_test_cat,predict_test),3)\n",
    "\n",
    "train_brier = round(brier_multi(y_train,train_p),3)\n",
    "test_brier = round(brier_multi(y_test,test_p),3)\n",
    "\n",
    "print([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])\n",
    "pd.DataFrame(np.array([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])).transpose().to_clipboard(index=False,header=False)\n",
    "# Look at the estimation results\n",
    "MNL_A.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa785eca-7d14-47fe-b65a-a8e6f435d357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the utility\n",
    "param_table = MNL_A.get_statsmodels_summary().tables[1].as_html()\n",
    "param_table = pd.read_html(param_table, header=0, index_col=0)[0]\n",
    "param_coef = param_table['coef']\n",
    "df_TR = df_long.loc[df_long['mode_id'] == 1,['TT','TC','HE','GA']]\n",
    "df_SM = df_long.loc[df_long['mode_id'] == 2,['TT','TC','HE','GA','SEAT']]\n",
    "df_CAR = df_long.loc[df_long['mode_id'] == 3,['TT','TC']]\n",
    "\n",
    "util_TR = param_table['coef']['ASC Train'] + np.dot(np.array(df_TR),np.array(param_table['coef'][['Travel Time(Train)','Travel Cost(All)','Headway(Train)','GA(Train)']]))\n",
    "util_SM = param_table['coef']['ASC Swissmetro'] + np.dot(np.array(df_SM),np.array(param_table['coef'][['Travel Time(Swissmetro)','Travel Cost(All)','Headway(Swissmetro)','GA(Swissmetro)','Airline Seat(Swissmetro)']]))\n",
    "util_CAR = np.dot(np.array(df_CAR),np.array(param_table['coef'][['Travel Time(Car)','Travel Cost(All)']]))\n",
    "\n",
    "print(util_TR.min(),util_TR.max(),util_SM.min(),util_SM.max(),util_CAR.min(),util_CAR.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408783a2-5cb7-4f19-a97a-84aafb6a2c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the individual-level VOT \n",
    "def first(x):\n",
    "    return(x.iloc[1,:])\n",
    "\n",
    "VOT_TR_A =  param_coef['Travel Time(Train)']/param_coef['Travel Cost(All)']\n",
    "VOT_SM_A =  param_coef['Travel Time(Swissmetro)']/param_coef['Travel Cost(All)']\n",
    "VOT_CAR_A =  param_coef['Travel Time(Car)']/param_coef['Travel Cost(All)']\n",
    "\n",
    "print(np.array([VOT_TR_A,VOT_SM_A,VOT_CAR_A,]).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33c721-6d15-491d-8021-b85042f477e8",
   "metadata": {},
   "source": [
    "## 2. MNL-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e436a5-04aa-49a9-9e42-e06c8360b110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternative-specific Utility Specification\n",
    "\n",
    "basic_specification = OrderedDict()\n",
    "basic_names = OrderedDict()\n",
    "\n",
    "basic_specification[\"intercept\"] = [1, 2]\n",
    "basic_names[\"intercept\"] = ['ASC Train',\n",
    "                            'ASC Swissmetro']\n",
    "\n",
    "basic_specification[\"TT\"] = [1, 2, 3]\n",
    "basic_names[\"TT\"] = ['Travel Time(Train)',\n",
    "                     'Travel Time(Swissmetro)',\n",
    "                     'Travel Time(Car)']\n",
    "\n",
    "basic_specification[\"TC\"] = [[1, 2, 3]]\n",
    "basic_names[\"TC\"] = ['Travel Cost(All)']\n",
    "                     \n",
    "\n",
    "basic_specification[\"HE\"] = [1, 2]\n",
    "basic_names[\"HE\"] = [\"Headway(Train)\",\n",
    "                     \"Headway(Swissmetro)\"]\n",
    "\n",
    "basic_specification[\"SEAT\"] = [2]\n",
    "basic_names[\"SEAT\"] = ['Airline Seat(Swissmetro)']\n",
    "\n",
    "basic_specification[\"GA\"] = [1,2]\n",
    "basic_names[\"GA\"] = ['GA(Train)',\n",
    "                     'GA(Swissmetro)']\n",
    "\n",
    "## First-order Interactions\n",
    "basic_specification[\"TTxAGE_2\"] = [1, 2, 3]\n",
    "basic_names[\"TTxAGE_2\"] = [\"TTxAGE_2(Train)\",\n",
    "                           \"TTxAGE_2(Swissmetro)\",\n",
    "                           \"TTxAGE_2(Car)\"]\n",
    "\n",
    "basic_specification[\"TTxAGE_3\"] = [1, 2, 3]\n",
    "basic_names[\"TTxAGE_3\"] = [\"TTxAGE_3(Train)\",\n",
    "                           \"TTxAGE_3(Swissmetro)\",\n",
    "                           \"TTxAGE_3(Car)\"]\n",
    "\n",
    "basic_specification[\"TTxAGE_4\"] = [1, 2, 3]\n",
    "basic_names[\"TTxAGE_4\"] = [\"TTxAGE_4(Train)\",\n",
    "                           \"TTxAGE_4(Swissmetro)\",\n",
    "                           \"TTxAGE_4(Car)\"]\n",
    "\n",
    "basic_specification[\"TTxAGE_5\"] = [1, 2, 3]\n",
    "basic_names[\"TTxAGE_5\"] = [\"TTxAGE_5(Train)\",\n",
    "                           \"TTxAGE_5(Swissmetro)\",\n",
    "                           \"TTxAGE_5(Car)\"]\n",
    "\n",
    "basic_specification[\"TTxINCOME_2\"] = [1, 2, 3]\n",
    "basic_names[\"TTxINCOME_2\"] = [\"TTxINCOME_2(Train)\",\n",
    "                              \"TTxINCOME_2(Swissmetro)\",\n",
    "                              \"TTxINCOME_2(Car)\"]\n",
    "\n",
    "basic_specification[\"TTxINCOME_3\"] = [1, 2, 3]\n",
    "basic_names[\"TTxINCOME_3\"] = [\"TTxINCOME_3(Train)\",\n",
    "                              \"TTxINCOME_3(Swissmetro)\",\n",
    "                              \"TTxINCOME_3(Car)\"]\n",
    "\n",
    "basic_specification[\"TTxPURPOSE_2\"] = [1, 2, 3]\n",
    "basic_names[\"TTxPURPOSE_2\"] = [\"TTxPURPOSE_2(Train)\",\n",
    "                              \"TTxPURPOSE_2(Swissmetro)\",\n",
    "                              \"TTxPURPOSE_2(Car)\"]\n",
    "\n",
    "basic_specification[\"TTxPURPOSE_3\"] = [1, 2, 3]\n",
    "basic_names[\"TTxPURPOSE_3\"] = [\"TTxPURPOSE_3(Train)\",\n",
    "                              \"TTxPURPOSE_3(Swissmetro)\",\n",
    "                              \"TTxPURPOSE_3(Car)\"]\n",
    "\n",
    "basic_specification[\"TTxPURPOSE_4\"] = [1, 2, 3]\n",
    "basic_names[\"TTxPURPOSE_4\"] = [\"TTxPURPOSE_4(Train)\",\n",
    "                              \"TTxPURPOSE_4(Swissmetro)\",\n",
    "                              \"TTxPURPOSE_4(Car)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3579ef5-465c-49ad-b130-011ffb84256e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "# Estimate the multinomial logit model (MNL)\n",
    "MNL_B = pl.create_choice_model(data=df_train,\n",
    "                                alt_id_col=custom_alt_id,\n",
    "                                obs_id_col=obs_id_column,\n",
    "                                choice_col=choice_column,\n",
    "                                specification=basic_specification,\n",
    "                                model_type=\"MNL\",\n",
    "                                names=basic_names)\n",
    "\n",
    "# Specify the initial values and method for the optimization.\n",
    "MNL_B.fit_mle(np.zeros(38))\n",
    "\n",
    "# Look at the estimation results\n",
    "MNL_B.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a6cc2-2565-4cc3-b590-f6df9d6c2cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the prediction metrics\n",
    "predict_train = np.argmax(MNL_B.predict(df_train).reshape(-1,3),axis=1)\n",
    "predict_test =  np.argmax(MNL_B.predict(df_test).reshape(-1,3),axis=1)\n",
    "\n",
    "y_train = np.array(df_train.CHOICE).reshape(-1,3)\n",
    "y_test = np.array(df_test.CHOICE).reshape(-1,3)\n",
    "y_train_cat = np.argmax(np.array(df_train.CHOICE).reshape(-1,3),axis=1)\n",
    "y_test_cat = np.argmax(np.array(df_test.CHOICE).reshape(-1,3),axis=1)\n",
    "\n",
    "\n",
    "train_p = MNL_B.predict(df_train).reshape(-1,3)\n",
    "test_p = MNL_B.predict(df_test).reshape(-1,3)\n",
    "\n",
    "test_NLL = round((F.cross_entropy(input=torch.log(tensor(test_p)),target=torch.Tensor(y_test)).numpy()).item(),3)\n",
    "train_NLL =  round((F.cross_entropy(input=torch.log(tensor(train_p)),target=torch.Tensor(y_train)).numpy()).item(),3)\n",
    "\n",
    "train_acc = round(accuracy_score(y_train_cat,predict_train),3)\n",
    "test_acc = round(accuracy_score(y_test_cat,predict_test),3)\n",
    "\n",
    "train_brier = round(brier_multi(y_train,train_p),3)\n",
    "test_brier = round(brier_multi(y_test,test_p),3)\n",
    "\n",
    "print([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])\n",
    "pd.DataFrame(np.array([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])).transpose().to_clipboard(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fed5a1-d12d-4c38-bfdc-36e8d3d89b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the individual-level VOT \n",
    "def first(x):\n",
    "    return(x.iloc[1,:])\n",
    "\n",
    "param_table = MNL_B.get_statsmodels_summary().tables[1].as_html()\n",
    "param_table = pd.read_html(param_table, header=0, index_col=0)[0]\n",
    "param_coef = param_table['coef']\n",
    "df_VOT_B = df_test[['ID','custom_id','AGE_2', 'AGE_3','AGE_4', 'AGE_5',\n",
    "                  'INCOME_2', 'INCOME_3', 'PURPOSE_2','PURPOSE_3', 'PURPOSE_4','AGES','INCOMES','PURPOSES']].groupby('custom_id').apply(first)\n",
    "df_VOT_B['intercept'] = np.ones(df_VOT_B.shape[0])\n",
    "VOT_TR_B = np.dot(\n",
    "        np.array(df_VOT_B[['intercept','AGE_2', 'AGE_3','AGE_4', 'AGE_5',\n",
    "                'INCOME_2', 'INCOME_3', 'PURPOSE_2','PURPOSE_3', 'PURPOSE_4']]),\n",
    "        np.array(param_coef[['Travel Time(Train)',\n",
    "                'TTxAGE_2(Train)','TTxAGE_3(Train)','TTxAGE_4(Train)','TTxAGE_5(Train)','TTxINCOME_2(Train)','TTxINCOME_3(Train)',\n",
    "                'TTxPURPOSE_2(Train)','TTxPURPOSE_3(Train)','TTxPURPOSE_4(Train)']]))\n",
    "\n",
    "VOT_SM_B = np.dot(\n",
    "        np.array(df_VOT_B[['intercept','AGE_2', 'AGE_3','AGE_4','AGE_5',\n",
    "                'INCOME_2', 'INCOME_3', 'PURPOSE_2','PURPOSE_3','PURPOSE_4']]),\n",
    "        np.array(param_coef[['Travel Time(Swissmetro)',\n",
    "                'TTxAGE_2(Swissmetro)','TTxAGE_3(Swissmetro)','TTxAGE_4(Swissmetro)','TTxAGE_5(Swissmetro)','TTxINCOME_2(Swissmetro)','TTxINCOME_3(Swissmetro)',\n",
    "                'TTxPURPOSE_2(Swissmetro)','TTxPURPOSE_3(Swissmetro)','TTxPURPOSE_4(Swissmetro)']]))\n",
    "\n",
    "VOT_CAR_B = np.dot(\n",
    "        np.array(df_VOT_B[['intercept','AGE_2', 'AGE_3','AGE_4', 'AGE_5',\n",
    "                'INCOME_2', 'INCOME_3', 'PURPOSE_2','PURPOSE_3', 'PURPOSE_4']]),\n",
    "        np.array(param_coef[['Travel Time(Car)',\n",
    "                'TTxAGE_2(Car)','TTxAGE_3(Car)','TTxAGE_4(Car)','TTxAGE_5(Car)','TTxINCOME_2(Car)','TTxINCOME_3(Car)',\n",
    "                'TTxPURPOSE_2(Car)','TTxPURPOSE_3(Car)','TTxPURPOSE_4(Car)']]))\n",
    "\n",
    "df_VOT_B['VOT_TR'] = VOT_TR_B / param_coef['Travel Cost(All)']\n",
    "df_VOT_B['VOT_SM'] = VOT_SM_B / param_coef['Travel Cost(All)']\n",
    "df_VOT_B['VOT_CAR'] = VOT_CAR_B / param_coef['Travel Cost(All)']\n",
    "\n",
    "df_VOT_B = df_VOT_B[['ID','VOT_TR','VOT_SM','VOT_CAR',\n",
    "                     'AGES','INCOMES','PURPOSES']].groupby(['AGES','INCOMES','PURPOSES']).mean()\n",
    "\n",
    "print(np.quantile(df_VOT_B['VOT_TR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\n",
    "print(np.quantile(df_VOT_B['VOT_SM'],(0.5,0.01,0.25,0.75,0.99)).round(3))\n",
    "print(np.quantile(df_VOT_B['VOT_CAR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337667a-09ec-4543-a0f7-3e4bda41be8e",
   "metadata": {},
   "source": [
    "## 3. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c3d01-c219-46f5-81a1-6aa4bfe97f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 1. DNN\n",
    "\n",
    "x_train = df_wide_train[[\n",
    "    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\n",
    "    'SM_TT','SM_TC','SM_HE','SM_SEATS',\n",
    "    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\n",
    "\n",
    "y_train = np.array(pd.get_dummies(df_wide_train['CHOICE']))\n",
    "\n",
    "\n",
    "x_test = df_wide_test[[\n",
    "    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\n",
    "    'SM_TT','SM_TC','SM_HE','SM_SEATS',\n",
    "    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\n",
    "\n",
    "y_test = np.array(pd.get_dummies(df_wide_test['CHOICE']))\n",
    "\n",
    "x_valid = df_wide_valid[[\n",
    "    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\n",
    "    'SM_TT','SM_TC','SM_HE','SM_SEATS',\n",
    "    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\n",
    "\n",
    "y_valid = np.array(pd.get_dummies(df_wide_valid['CHOICE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c0f8d-c597-4131-8905-19b42819db55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 2. Define the Original DNN\n",
    "\n",
    "def build_DNN(num_layers,num_neurons,drop_rate,learning_rate):\n",
    "\n",
    "    img = Input(shape=x_train.shape[1],name=\"main_input\")   \n",
    "    h = Dense(num_neurons,activation='relu')(img)\n",
    "    h = Dropout(drop_rate)(h)\n",
    "    for num_layer in range(num_layers-1):\n",
    "        h = Dense(num_neurons,activation='relu')(h)\n",
    "        h = Dropout(drop_rate)(h)\n",
    "    util = Dense(1)(h)\n",
    "    \n",
    "    ## Alternative-specific utility\n",
    "    util_ALT1 = Dense(int(num_neurons*0.5))(h)\n",
    "    util_ALT1 = Dropout(drop_rate)(util_ALT1)\n",
    "    util_ALT1 = Dense(1,name='output_TR')(util_ALT1)\n",
    "    \n",
    "    util_ALT2 = Dense(int(num_neurons*0.5))(h)\n",
    "    util_ALT2 = Dropout(drop_rate)(util_ALT2)\n",
    "    util_ALT2 = Dense(1,name='output_SM')(util_ALT2)\n",
    "       \n",
    "    util_ALT3 = Dense(int(num_neurons*0.5))(h)\n",
    "    util_ALT3 = Dropout(drop_rate)(util_ALT3)\n",
    "    util_ALT3 = Dense(1,name='output_CAR')(util_ALT3)\n",
    "\n",
    "    out_prob =  tf.keras.layers.Softmax(name='out_prob')(Concatenate()([util_ALT1,util_ALT2,util_ALT3]))\n",
    "    \n",
    "    model = Model(img,[out_prob,util_ALT1,util_ALT2,util_ALT3])  \n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=['categorical_crossentropy',None,None,None],\n",
    "                  metrics=['accuracy',None,None,None])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c969d-3912-4ff8-a397-16fc9abeed55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0,1):\n",
    "    \n",
    "    num_layers = 3\n",
    "    num_neurons = 200\n",
    "    drop_rate = 0.005\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    DNN = build_DNN(num_layers,num_neurons,drop_rate,learning_rate)\n",
    "\n",
    "    batch_size = 128\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, verbose=1, patience=20)\n",
    "    history = DNN.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        shuffle=True,\n",
    "        epochs = 200,\n",
    "        batch_size = batch_size,\n",
    "        validation_data=[x_valid,y_valid],\n",
    "        callbacks = [es],\n",
    "        verbose=0)\n",
    "\n",
    "    DNN.save_weights('C:/Users/euijin/Documents/DNN_Models_Revision/DNN_FUL_'+str(i))\n",
    "    #DNN.load_weights('C:/Users/euijin/Documents/DNN_Models/DNN_FUL_'+str(i))\n",
    "\n",
    "\n",
    "\n",
    "    # Get the prediction metrics\n",
    "    test_p = DNN.predict(x_test,verbose=0)\n",
    "    train_p = DNN.predict(x_train,verbose=0)\n",
    "\n",
    "    predict_train = np.argmax(train_p[0],axis=1)\n",
    "    predict_test =  np.argmax(test_p[0],axis=1)\n",
    "\n",
    "    y_train = np.array(df_train.CHOICE).reshape(-1,3)\n",
    "    y_test = np.array(df_test.CHOICE).reshape(-1,3)\n",
    "    y_train_cat = np.argmax(np.array(df_train.CHOICE).reshape(-1,3),axis=1)\n",
    "    y_test_cat = np.argmax(np.array(df_test.CHOICE).reshape(-1,3),axis=1)\n",
    "\n",
    "    train_acc = round(accuracy_score(y_train_cat,predict_train),3)\n",
    "    test_acc = round(accuracy_score(y_test_cat,predict_test),3)\n",
    "\n",
    "    train_brier = round(brier_multi(y_train,train_p[0]),3)\n",
    "    test_brier = round(brier_multi(y_test,test_p[0]),3)\n",
    "\n",
    "    print([train_acc,test_acc,train_brier,test_brier])\n",
    "\n",
    "\n",
    "    ## Plot the PD and ICE\n",
    "    from sklearn.inspection import partial_dependence\n",
    "    from sklearn.inspection import PartialDependenceDisplay\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    class DNNFunction(BaseEstimator, ClassifierMixin):\n",
    "        def fit(self, X, y):\n",
    "            self.classes_ = unique_labels(y)\n",
    "            return self\n",
    "\n",
    "        def predict_proba(self, X):\n",
    "            self.proba_,_,_,_ = DNN.predict(X,verbose=0)\n",
    "            return np.array(self.proba_)\n",
    "\n",
    "        def decision_function(self, X):\n",
    "            self.utility_ = np.array(DNN.predict(X)[1:4]).transpose().reshape(-1,3)\n",
    "            return self.utility_\n",
    "\n",
    "    DNN_ALL = DNNFunction()\n",
    "    DNN_ALL.fit(x_train,y_train)\n",
    "\n",
    "    common_params = {\n",
    "        \"subsample\": 0.99999,\n",
    "        \"n_jobs\": 1,\n",
    "        \"grid_resolution\": 20,\n",
    "        \"centered\": True,\n",
    "        \"random_state\": 1,\n",
    "        \"response_method\": 'decision_function',\n",
    "        \"percentiles\": (0.01, 0.99),\n",
    "\n",
    "    }\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\n",
    "\n",
    "    display_0 = PartialDependenceDisplay.from_estimator(\n",
    "        DNN_ALL,\n",
    "        x_train,\n",
    "        features=['TRAIN_TT','TRAIN_TC'], # TC,TT\n",
    "        kind=[\"both\",\"both\"],\n",
    "        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\n",
    "        ax=ax,\n",
    "        target = 0,\n",
    "        **common_params,\n",
    "    )\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\n",
    "\n",
    "    display_1 = PartialDependenceDisplay.from_estimator(\n",
    "        DNN_ALL,\n",
    "        x_train,\n",
    "        features=['SM_TT','SM_TC'], # TC,TT\n",
    "        kind=[\"both\",\"both\"],\n",
    "        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\n",
    "        ax=ax,\n",
    "        target = 1,\n",
    "        **common_params,\n",
    "    )\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\n",
    "\n",
    "    display_2 = PartialDependenceDisplay.from_estimator(\n",
    "        DNN_ALL,\n",
    "        x_train,\n",
    "        features=['CAR_TT','CAR_TC'], # TC,TT\n",
    "        kind=[\"both\",\"both\"],\n",
    "        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\n",
    "        ax=ax,\n",
    "        target = 2,\n",
    "        **common_params,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    ## Population-level Parameters\n",
    "\n",
    "    PD_DNN_TT_TR = display_0.pd_results[0]['average'][0]\n",
    "    PD_DNN_TC_TR = display_0.pd_results[1]['average'][0] \n",
    "\n",
    "    PD_DNN_TT_SM = display_1.pd_results[0]['average'][1]\n",
    "    PD_DNN_TC_SM = display_1.pd_results[1]['average'][1] \n",
    "\n",
    "    PD_DNN_TT_CAR = display_2.pd_results[0]['average'][2]\n",
    "    PD_DNN_TC_CAR = display_2.pd_results[1]['average'][2] \n",
    "\n",
    "    PD_values_TT_TR = display_0.pd_results[0]['values'][0]\n",
    "    PD_values_TC_TR = display_0.pd_results[1]['values'][0]\n",
    "\n",
    "    PD_values_TT_SM = display_1.pd_results[0]['values'][0]\n",
    "    PD_values_TC_SM = display_1.pd_results[1]['values'][0]\n",
    "\n",
    "    PD_values_TT_CAR = display_2.pd_results[0]['values'][0]\n",
    "    PD_values_TC_CAR = display_2.pd_results[1]['values'][0]\n",
    "\n",
    "    beta_DNN_TT_TR = (np.diff(PD_DNN_TT_TR)/np.diff(PD_values_TT_TR))\n",
    "    beta_DNN_TC_TR = (np.diff(PD_DNN_TC_TR)/np.diff(PD_values_TC_TR))\n",
    "\n",
    "    beta_DNN_TT_SM = (np.diff(PD_DNN_TT_SM)/np.diff(PD_values_TT_SM))\n",
    "    beta_DNN_TC_SM = (np.diff(PD_DNN_TC_SM)/np.diff(PD_values_TC_SM))\n",
    "\n",
    "    beta_DNN_TT_CAR = (np.diff(PD_DNN_TT_CAR)/np.diff(PD_values_TT_CAR))\n",
    "    beta_DNN_TC_CAR = (np.diff(PD_DNN_TC_CAR)/np.diff(PD_values_TC_CAR))\n",
    "\n",
    "\n",
    "    TC_TR_IDX =(beta_DNN_TC_TR != 0)\n",
    "    TC_SM_IDX =(beta_DNN_TC_SM != 0)\n",
    "    TC_CAR_IDX =(beta_DNN_TC_CAR != 0)\n",
    "\n",
    "    beta_DNN_TC_TR = beta_DNN_TC_TR[TC_TR_IDX]\n",
    "    beta_DNN_TC_SM = beta_DNN_TC_SM[TC_SM_IDX]\n",
    "    beta_DNN_TC_CAR = beta_DNN_TC_CAR[TC_CAR_IDX]\n",
    "\n",
    "    beta_DNN_TT_TR = beta_DNN_TT_TR[TC_TR_IDX]\n",
    "    beta_DNN_TT_SM = beta_DNN_TT_SM[TC_SM_IDX]\n",
    "    beta_DNN_TT_CAR = beta_DNN_TT_CAR[TC_CAR_IDX]\n",
    "\n",
    "\n",
    "    beta_DNN_VOT_POP_TR = np.median(compute_VOT_POP(beta_DNN_TT_TR,beta_DNN_TC_TR))\n",
    "    beta_DNN_VOT_POP_SM = np.median(compute_VOT_POP(beta_DNN_TT_SM,beta_DNN_TC_SM))\n",
    "    beta_DNN_VOT_POP_CAR = np.median(compute_VOT_POP(beta_DNN_TT_CAR,beta_DNN_TC_CAR))\n",
    "\n",
    "\n",
    "    ## Individual-level Parameters\n",
    "\n",
    "    ICE_DNN_TT_TR = display_0.pd_results[0]['individual'][0]\n",
    "    ICE_DNN_TC_TR = display_0.pd_results[1]['individual'][0]\n",
    "\n",
    "    ICE_DNN_TT_SM = display_1.pd_results[0]['individual'][1]\n",
    "    ICE_DNN_TC_SM = display_1.pd_results[1]['individual'][1]\n",
    "\n",
    "    ICE_DNN_TT_CAR = display_2.pd_results[0]['individual'][2]\n",
    "    ICE_DNN_TC_CAR = display_2.pd_results[1]['individual'][2]\n",
    "\n",
    "    beta_DNN_TT_TR_ICE = (np.diff(ICE_DNN_TT_TR)/np.diff(PD_values_TT_TR))\n",
    "    beta_DNN_TC_TR_ICE = (np.diff(ICE_DNN_TC_TR)/np.diff(PD_values_TC_TR))\n",
    "    beta_DNN_VOT_TR_ICE = compute_VOT_IND(beta_DNN_TT_TR_ICE[:,TC_TR_IDX],beta_DNN_TC_TR_ICE[:,TC_TR_IDX])\n",
    "\n",
    "    beta_DNN_TT_SM_ICE = (np.diff(ICE_DNN_TT_SM)/np.diff(PD_values_TT_SM))\n",
    "    beta_DNN_TC_SM_ICE = (np.diff(ICE_DNN_TC_SM)/np.diff(PD_values_TC_SM))\n",
    "    beta_DNN_VOT_SM_ICE = compute_VOT_IND(beta_DNN_TT_SM_ICE[:,TC_SM_IDX],beta_DNN_TC_SM_ICE[:,TC_SM_IDX])\n",
    "\n",
    "\n",
    "    beta_DNN_TT_CAR_ICE = (np.diff(ICE_DNN_TT_CAR)/np.diff(PD_values_TT_CAR))\n",
    "    beta_DNN_TC_CAR_ICE = (np.diff(ICE_DNN_TC_CAR)/np.diff(PD_values_TC_CAR))\n",
    "    beta_DNN_VOT_CAR_ICE = compute_VOT_IND(beta_DNN_TT_CAR_ICE[:,TC_CAR_IDX],beta_DNN_TC_CAR_ICE[:,TC_CAR_IDX])\n",
    "\n",
    "\n",
    "    # Estimation # If we use the median value, we don't need to care about the outlier or somethings.\n",
    "\n",
    "    x_test_E = x_train[['AGES','INCOMES','PURPOSES']].reset_index(drop=True)\n",
    "    x_test_E['VOT_TR'] = beta_DNN_VOT_TR_ICE\n",
    "    x_test_E['VOT_SM'] = beta_DNN_VOT_SM_ICE\n",
    "    x_test_E['VOT_CAR'] = beta_DNN_VOT_CAR_ICE\n",
    "\n",
    "    VOT_DNN_IND = x_test_E[['AGES','INCOMES','PURPOSES','VOT_TR','VOT_SM','VOT_CAR']].groupby(['AGES','INCOMES','PURPOSES']).median()\n",
    "\n",
    "\n",
    "    print(np.quantile(VOT_DNN_IND['VOT_TR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\n",
    "    print(np.quantile(VOT_DNN_IND['VOT_SM'],(0.5,0.01,0.25,0.75,0.99)).round(3))\n",
    "    print(np.quantile(VOT_DNN_IND['VOT_CAR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\n",
    "\n",
    "\n",
    "    # Estimation results\n",
    "    PRED_PERF = np.array([train_acc,test_acc,train_brier,test_brier]).transpose()\n",
    "    EST_RESULTS = [PRED_PERF,VOT_DNN_IND]\n",
    "    with open(\"C:/Users/euijin/Documents/DNN_Results_Revision/FUL_RESULTS_FF_\"+str(i), \"wb\") as f:\n",
    "        pickle.dump(EST_RESULTS, f)\n",
    "    #save_clipboard(np.concatenate([PRED_PERF,VOT_DNN_IND.median(axis=0).round(3)])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd58e2fd-6260-4a19-ad2b-3de6fb26c654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "## Results analysis\n",
    "PRED_PERF_S = []\n",
    "VOT_DNN_IND_S = []\n",
    "VOT_DNN_index = []\n",
    "\n",
    "for i in range(1):\n",
    "    with open(\"C:/Users/euijin/Documents/DNN_Results_Revision/FUL_RESULTS_FF_\"+str(i),'rb') as f:\n",
    "        temp = pickle.load(f)\n",
    "    PRED_PERF_S.append(temp[0])\n",
    "    VOT_DNN_IND_S.append(temp[1])\n",
    "    VOT_DNN_index.append(temp[1].index.values)\n",
    "    \n",
    "\n",
    "## Predictability    \n",
    "PRED_PERF_S = np.array(PRED_PERF_S)\n",
    "A = np.concatenate([PRED_PERF_S.mean(axis=0).round(3).reshape(-1,1),PRED_PERF_S.std(axis=0).round(3).reshape(-1,1)],axis=1)\n",
    "\n",
    "\n",
    "## Interpretability\n",
    "IND_stats = []\n",
    "VOT_DNN_IND_S_TR = []\n",
    "VOT_DNN_IND_S_SM = []\n",
    "VOT_DNN_IND_S_CAR = []\n",
    "\n",
    "for i in range(len(VOT_DNN_IND_S)):\n",
    "    \n",
    "    inter_IDX_IND = VOT_DNN_IND_S[i].index.isin(np.intersect1d(VOT_DNN_IND_S[i].index,df_VOT_B.index))\n",
    "        \n",
    "    TRIND = np.quantile(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,0],(0.5,0.01,0.25,0.75,0.99)).round(3)\n",
    "    SMIND = np.quantile(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,1],(0.5,0.01,0.25,0.75,0.99)).round(3)\n",
    "    CARIND = np.quantile(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,2],(0.5,0.01,0.25,0.75,0.99)).round(3)   \n",
    "    ALLIND = np.concatenate([TRIND,SMIND,CARIND])   \n",
    "    IND_stats.append(ALLIND)\n",
    "    \n",
    "    VOT_DNN_IND_S_TR.append(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,0])\n",
    "    VOT_DNN_IND_S_SM.append(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,1])\n",
    "    VOT_DNN_IND_S_CAR.append(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,2])\n",
    "      \n",
    "B = np.array(IND_stats)\n",
    "B = np.concatenate([np.median(B,axis=0).round(3).reshape(-1,1),\n",
    "                    stats.iqr(B,axis=0).round(3).reshape(-1,1)],axis=1)\n",
    "\n",
    "\n",
    "VOT_DNN_IND_S_TR = np.concatenate(VOT_DNN_IND_S_TR)\n",
    "VOT_DNN_IND_S_SM = np.concatenate(VOT_DNN_IND_S_SM)\n",
    "VOT_DNN_IND_S_CAR = np.concatenate(VOT_DNN_IND_S_CAR)\n",
    "\n",
    "EST_RESULTS_DNN = np.vstack([B,A])\n",
    "save_clipboard(EST_RESULTS_DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f402409-c7ca-45c9-a6f7-f037a5bc66ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T10:25:14.611889Z",
     "iopub.status.busy": "2022-10-10T10:25:14.611889Z",
     "iopub.status.idle": "2022-10-10T10:25:14.621857Z",
     "shell.execute_reply": "2022-10-10T10:25:14.621857Z",
     "shell.execute_reply.started": "2022-10-10T10:25:14.611889Z"
    }
   },
   "source": [
    "## 4. TCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76100f-9001-4e89-b3ec-935f035a3915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 2. BCDNN\n",
    "\n",
    "x_train = df_wide_train[[\n",
    "    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\n",
    "    'SM_TT','SM_TC','SM_HE','SM_SEATS',\n",
    "    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\n",
    "\n",
    "y_train = np.array(pd.get_dummies(df_wide_train['CHOICE']))\n",
    "\n",
    "\n",
    "x_test = df_wide_test[[\n",
    "    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\n",
    "    'SM_TT','SM_TC','SM_HE','SM_SEATS',\n",
    "    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\n",
    "\n",
    "y_test = np.array(pd.get_dummies(df_wide_test['CHOICE']))\n",
    "\n",
    "x_valid = df_wide_valid[[\n",
    "    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\n",
    "    'SM_TT','SM_TC','SM_HE','SM_SEATS',\n",
    "    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\n",
    "\n",
    "y_valid = np.array(pd.get_dummies(df_wide_valid['CHOICE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d61c6-7791-4f91-ad91-668a44994dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hyperparameters of Lattice Networks\n",
    "\n",
    "def build_LatDNN(TRAIN_TT_KP,TRAIN_TT_LS,TRAIN_TC_KP,TRAIN_TC_LS,TRAIN_HE_KP,TRAIN_HE_LS,SM_TT_KP,SM_TT_LS,SM_TC_KP,SM_TC_LS,SM_HE_KP,SM_HE_LS,\n",
    "                 CAR_TT_KP,CAR_TT_LS,CAR_TC_KP,CAR_TC_LS,SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS,ACT_TR,ACT_SM,ACT_CAR):\n",
    "    \n",
    "    non_split_input = Input(shape=x_train.shape[1], name='non_split_input',dtype='float32')\n",
    "    \n",
    "    # Alternative-specific Lattice inputs\n",
    "    lattice_inputs_TR = []\n",
    "    lattice_inputs_SM = []\n",
    "    lattice_inputs_CAR = []\n",
    "    \n",
    "    # TRAIN_TT\n",
    "    TRAIN_TT_input = tf.reshape(non_split_input[:,0],(-1,1))\n",
    "    \n",
    "    ## TRAIN_TT to TR\n",
    "    TRAIN_TT_TR_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['TRAIN_TT'].min(), x_train['TRAIN_TT'].max(), num=TRAIN_TT_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=TRAIN_TT_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='decreasing', ## Sign constraints \n",
    "    name='TRAIN_TT_TR_Calib',\n",
    "    )(TRAIN_TT_input)\n",
    "\n",
    "    ## TRAIN_TT to Others\n",
    "    TRAIN_TT_OTH_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['TRAIN_TT'].min(), x_train['TRAIN_TT'].max(), num=TRAIN_TT_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=TRAIN_TT_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='increasing',\n",
    "    name='TRAIN_TT_OTH_Calib',\n",
    "    )(TRAIN_TT_input)\n",
    "   \n",
    "\n",
    "    lattice_inputs_TR.append(TRAIN_TT_TR_calibrator)\n",
    "    # lattice_inputs_SM.append(TRAIN_TT_OTH_calibrator)\n",
    "    # lattice_inputs_CAR.append(TRAIN_TT_OTH_calibrator)      \n",
    "    \n",
    "    \n",
    "    # TRAIN_TC\n",
    "    TRAIN_TC_input = tf.reshape(non_split_input[:,1],(-1,1))\n",
    "    \n",
    "    ## TRAIN_TC to TR\n",
    "    TRAIN_TC_TR_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['TRAIN_TC'].min(), x_train['TRAIN_TC'].max(), num=TRAIN_TC_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=TRAIN_TC_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='decreasing', ## Sign constraints \n",
    "    name='TRAIN_TC_TR_Calib',\n",
    "    )(TRAIN_TC_input)\n",
    "\n",
    "\n",
    "    ## TRAIN_TC to Others\n",
    "    TRAIN_TC_OTH_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['TRAIN_TC'].min(), x_train['TRAIN_TC'].max(), num=TRAIN_TC_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=TRAIN_TC_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='increasing',\n",
    "    name='TRAIN_TC_OTH_Calib',\n",
    "    )(TRAIN_TC_input)\n",
    "    \n",
    "    lattice_inputs_TR.append(TRAIN_TC_TR_calibrator)\n",
    "    # lattice_inputs_SM.append(TRAIN_TC_OTH_calibrator)\n",
    "    # lattice_inputs_CAR.append(TRAIN_TC_OTH_calibrator)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # TRAIN_HE\n",
    "    TRAIN_HE_input = tf.reshape(non_split_input[:,2],(-1,1))\n",
    "    \n",
    "    ## TRAIN_HE to TR\n",
    "    TRAIN_HE_TR_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['TRAIN_HE'].min(), x_train['TRAIN_HE'].max(), num=TRAIN_HE_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=TRAIN_HE_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='decreasing', ## Sign constraints \n",
    "    name='TRAIN_HE_TR_Calib',\n",
    "    )(TRAIN_HE_input)\n",
    "\n",
    "\n",
    "    ## TRAIN_HE to Others\n",
    "    TRAIN_HE_OTH_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['TRAIN_HE'].min(), x_train['TRAIN_HE'].max(), num=TRAIN_HE_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=TRAIN_HE_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='increasing', ## Sign constraints \n",
    "    name='TRAIN_HE_OTH_Calib',\n",
    "    )(TRAIN_HE_input)\n",
    "   \n",
    "    lattice_inputs_TR.append(TRAIN_HE_TR_calibrator)\n",
    "    # lattice_inputs_SM.append(TRAIN_HE_OTH_calibrator)\n",
    "    # lattice_inputs_CAR.append(TRAIN_HE_OTH_calibrator)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Swissmetro_TT\n",
    "    SM_TT_input = tf.reshape(non_split_input[:,3],(-1,1))\n",
    "    \n",
    "    ## SM_TT to SM\n",
    "    SM_TT_SM_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['SM_TT'].min(), x_train['SM_TT'].max(), num=SM_TT_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=SM_TT_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='decreasing', ## Sign constraints \n",
    "    name='SM_TT_SM_Calib',\n",
    "    )(SM_TT_input)\n",
    "\n",
    "    ## SM_TT to Others\n",
    "    SM_TT_OTH_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['SM_TT'].min(), x_train['SM_TT'].max(), num=SM_TT_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=SM_TT_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='increasing', ## Sign constraints \n",
    "    name='SM_TT_OTH_Calib',\n",
    "    )(SM_TT_input)\n",
    "    lattice_inputs_SM.append(SM_TT_SM_calibrator)\n",
    "    # lattice_inputs_TR.append(SM_TT_OTH_calibrator)\n",
    "    # lattice_inputs_CAR.append(SM_TT_OTH_calibrator)\n",
    "    \n",
    "    \n",
    "    # Swissmetro_TC\n",
    "    SM_TC_input = tf.reshape(non_split_input[:,4],(-1,1))\n",
    "    \n",
    "    ## SM_TC to SM\n",
    "    SM_TC_SM_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['SM_TC'].min(), x_train['SM_TC'].max(), num=SM_TC_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=SM_TC_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='decreasing', ## Sign constraints \n",
    "    name='SM_TC_SM_Calib',\n",
    "    )(SM_TC_input)\n",
    "\n",
    "    ## SM_TC to Others\n",
    "    SM_TC_OTH_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['SM_TC'].min(), x_train['SM_TC'].max(), num=SM_TC_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=SM_TC_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='increasing', ## Sign constraints \n",
    "    name='SM_TC_OTH_Calib',\n",
    "    )(SM_TC_input)\n",
    "\n",
    "    lattice_inputs_SM.append(SM_TC_SM_calibrator)\n",
    "    # lattice_inputs_TR.append(SM_TC_OTH_calibrator)\n",
    "    # lattice_inputs_CAR.append(SM_TC_OTH_calibrator)\n",
    "    \n",
    "  \n",
    "\n",
    "    # Swissmetro_HE\n",
    "    SM_HE_input = tf.reshape(non_split_input[:,5],(-1,1))\n",
    "    \n",
    "    ## SM_HE to SM\n",
    "    SM_HE_SM_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['SM_HE'].min(), x_train['SM_HE'].max(), num=SM_HE_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=SM_HE_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='decreasing', ## Sign constraints \n",
    "    name='SM_HE_SM_Calib',\n",
    "    )(SM_HE_input)\n",
    "\n",
    "    ## SM_HE to Others\n",
    "    SM_HE_OTH_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['SM_HE'].min(), x_train['SM_HE'].max(), num=SM_HE_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=SM_HE_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='increasing', ## Sign constraints \n",
    "    name='SM_HE_OTH_Calib',\n",
    "    )(SM_HE_input)\n",
    "\n",
    "    lattice_inputs_SM.append(SM_HE_SM_calibrator)\n",
    "    # lattice_inputs_TR.append(SM_HE_OTH_calibrator)\n",
    "    # lattice_inputs_CAR.append(SM_HE_OTH_calibrator)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # CAR_TT\n",
    "    CAR_TT_input = tf.reshape(non_split_input[:,7],(-1,1))\n",
    "    \n",
    "    ## CAR_TT to CAR\n",
    "    CAR_TT_CAR_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['CAR_TT'].min(), x_train['CAR_TT'].max(), num=CAR_TT_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=CAR_TT_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='decreasing', ## Sign constraints \n",
    "    name='CAR_TT_CAR_Calib',\n",
    "    )(CAR_TT_input)\n",
    "\n",
    "    ## CAR_TT to Others\n",
    "    CAR_TT_OTH_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['CAR_TT'].min(), x_train['CAR_TT'].max(), num=CAR_TT_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=CAR_TT_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='increasing', ## Sign constraints \n",
    "    name='CAR_TT_OTH_Calib',\n",
    "    )(CAR_TT_input)\n",
    "    lattice_inputs_CAR.append(CAR_TT_CAR_calibrator)\n",
    "    # lattice_inputs_TR.append(CAR_TT_OTH_calibrator)\n",
    "    # lattice_inputs_SM.append(CAR_TT_OTH_calibrator)\n",
    "    \n",
    "    \n",
    "    # CAR_TC\n",
    "    CAR_TC_input = tf.reshape(non_split_input[:,8],(-1,1))\n",
    "    \n",
    "    ## CAR_TC to CAR\n",
    "    CAR_TC_CAR_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['CAR_TC'].min(), x_train['CAR_TC'].max(), num=CAR_TC_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=CAR_TC_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='decreasing', ## Sign constraints \n",
    "    name='CAR_TC_CAR_Calib',\n",
    "    )(CAR_TC_input)\n",
    "\n",
    "    ## CAR_TC to Others\n",
    "    CAR_TC_OTH_calibrator = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(\n",
    "        x_train['CAR_TC'].min(), x_train['CAR_TC'].max(), num=CAR_TC_KP),\n",
    "    dtype=tf.float32,\n",
    "    output_min=0.0,\n",
    "    output_max=CAR_TC_LS - 1.0,\n",
    "    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\n",
    "    monotonicity='increasing', ## Sign constraints \n",
    "    name='CAR_TC_OTH_Calib',\n",
    "    )(CAR_TC_input)\n",
    "    lattice_inputs_CAR.append(CAR_TC_CAR_calibrator)\n",
    "    # lattice_inputs_TR.append(CAR_TC_OTH_calibrator)\n",
    "    # lattice_inputs_SM.append(CAR_TC_OTH_calibrator)\n",
    "       \n",
    "       \n",
    "    \n",
    "    \n",
    "    ## Non-monotonic attributes\n",
    "    \n",
    "    ## SM_SEATS\n",
    "    SM_SEATS_input = tf.reshape(non_split_input[:,6],(-1,1))\n",
    "    SM_SEATS_calibrator = tfl.layers.CategoricalCalibration(\n",
    "        num_buckets=2,\n",
    "        output_min=0.0,\n",
    "        output_max=SM_SEATS_LS - 1.0,\n",
    "        name='SM_SEATS_calib',\n",
    "    )(SM_SEATS_input)\n",
    "    lattice_inputs_CAR.append(SM_SEATS_calibrator)\n",
    "    lattice_inputs_TR.append(SM_SEATS_calibrator)\n",
    "    lattice_inputs_SM.append(SM_SEATS_calibrator)\n",
    "\n",
    "    ## GA\n",
    "    GA_input = tf.reshape(non_split_input[:,9],(-1,1))\n",
    "    GA_calibrator = tfl.layers.CategoricalCalibration(\n",
    "        num_buckets=2,\n",
    "        output_min=0.0,\n",
    "        output_max=GA_LS - 1.0,\n",
    "        name='GA_SM_calib',\n",
    "    )(GA_input)\n",
    "    lattice_inputs_CAR.append(GA_calibrator)\n",
    "    lattice_inputs_TR.append(GA_calibrator)\n",
    "    lattice_inputs_SM.append(GA_calibrator)\n",
    "    \n",
    "    ## AGE\n",
    "    AGE_input = tf.reshape(non_split_input[:,10],(-1,1))\n",
    "    AGE_calibrator = tfl.layers.CategoricalCalibration(\n",
    "        num_buckets=5,\n",
    "        output_min=0.0,\n",
    "        output_max=AGE_LS - 1.0,\n",
    "        name='AGE_SM_calib',\n",
    "    )(AGE_input)\n",
    "    lattice_inputs_CAR.append(AGE_calibrator)\n",
    "    lattice_inputs_TR.append(AGE_calibrator)\n",
    "    lattice_inputs_SM.append(AGE_calibrator)\n",
    "\n",
    "    ## INCOME\n",
    "    INCOME_input = tf.reshape(non_split_input[:,11],(-1,1))\n",
    "    INCOME_calibrator = tfl.layers.CategoricalCalibration(\n",
    "        num_buckets=3,\n",
    "        output_min=0.0,\n",
    "        output_max=INCOME_LS - 1.0,\n",
    "        name='INCOME_calib',\n",
    "    )(INCOME_input)\n",
    "    lattice_inputs_CAR.append(INCOME_calibrator)\n",
    "    lattice_inputs_TR.append(INCOME_calibrator)\n",
    "    lattice_inputs_SM.append(INCOME_calibrator)\n",
    "    \n",
    "    \n",
    "    ## PURPOSE\n",
    "    PURPOSE_input = tf.reshape(non_split_input[:,12],(-1,1))\n",
    "    PURPOSE_calibrator = tfl.layers.CategoricalCalibration(\n",
    "        num_buckets=4,\n",
    "        output_min=0.0,\n",
    "        output_max=PURPOSE_LS - 1.0,\n",
    "        name='PURPOSE_calib',\n",
    "    )(PURPOSE_input)\n",
    "    lattice_inputs_CAR.append(PURPOSE_calibrator)\n",
    "    lattice_inputs_TR.append(PURPOSE_calibrator)\n",
    "    lattice_inputs_SM.append(PURPOSE_calibrator)   \n",
    "    \n",
    "\n",
    "    ### Non-lienarly fuse the outputs of calibrator\n",
    "    util_ALT1 = tfl.layers.Lattice(   \n",
    "    lattice_sizes=[TRAIN_TT_LS,TRAIN_TC_LS,TRAIN_HE_LS,\n",
    "                   SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS],\n",
    "    monotonicities=[\n",
    "        'increasing', 'increasing','increasing',\n",
    "        'none','none','none','none','none'],\n",
    "\n",
    "    output_min=0,\n",
    "    output_max=1,\n",
    "    # output_min=-100,\n",
    "    # output_max=100,\n",
    "    name='lattice_TR',\n",
    "    )(lattice_inputs_TR)\n",
    "    \n",
    "    \n",
    "    ### Non-lienarly fuse the outputs of calibrator\n",
    "    util_ALT2 = tfl.layers.Lattice(   \n",
    "    lattice_sizes=[SM_TT_LS,SM_TC_LS,SM_HE_LS,\n",
    "                   SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS],\n",
    "    monotonicities=[\n",
    "        'increasing', 'increasing','increasing',\n",
    "        'none','none','none','none','none'],\n",
    "\n",
    "    output_min=0,\n",
    "    output_max=1,\n",
    "    # output_min=-100,\n",
    "    # output_max=100,\n",
    "    name='lattice_SM',\n",
    "    )(lattice_inputs_SM)\n",
    "    \n",
    "    ## Non-lienarly fuse the outputs of calibrator\n",
    "    util_ALT3= tfl.layers.Lattice(   \n",
    "    lattice_sizes=[CAR_TT_LS,CAR_TC_LS,\n",
    "                   SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS],\n",
    "    monotonicities=[\n",
    "        'increasing', 'increasing',\n",
    "        'none','none','none','none','none'],\n",
    "\n",
    "    output_min=0,\n",
    "    output_max=1,\n",
    "    # output_min=-100,\n",
    "    # output_max=100,\n",
    "    name='lattice_CAR',\n",
    "    )(lattice_inputs_CAR)\n",
    "    \n",
    "    ## Output PWLCalibrator is the key of improving predictability\n",
    "    util_ALT1 = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(0.0, 1.0, ACT_TR),\n",
    "    output_min=-100,\n",
    "    output_max=100,\n",
    "    name='output_TR')(util_ALT1)\n",
    "  \n",
    "    util_ALT2 = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(0.0, 1.0, ACT_SM),\n",
    "    output_min=-100,\n",
    "    output_max=100,\n",
    "    name='output_SM')(util_ALT2)\n",
    "    \n",
    "    \n",
    "    util_ALT3 = tfl.layers.PWLCalibration(\n",
    "    input_keypoints=np.linspace(0.0, 1.0, ACT_CAR),\n",
    "    output_min=-100,\n",
    "    output_max=100,\n",
    "    name='output_CAR')(util_ALT3)\n",
    "\n",
    "    out_prob =  tf.keras.layers.Softmax(name='out_prob')(Concatenate()([util_ALT1,util_ALT2,util_ALT3]))\n",
    "    \n",
    "    model = Model(non_split_input,[out_prob,util_ALT1,util_ALT2,util_ALT3])   \n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "           \n",
    "    model.compile(optimizer=optimizer, loss=['categorical_crossentropy',None,None,None],\n",
    "                  metrics=['accuracy',None,None,None])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5cc15d9-ebcb-4e5e-b379-94a5108d4441",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "## Repeated Experiments\n",
    "for i in range(1):\n",
    "\n",
    "    TRAIN_TT_KP = 10\n",
    "    SM_TT_KP = 30\n",
    "    CAR_TT_KP = 10\n",
    "    \n",
    "    TRAIN_TC_KP = 30\n",
    "    SM_TC_KP = 30\n",
    "    CAR_TC_KP = 10\n",
    "    TRAIN_HE_KP = 10\n",
    "    SM_HE_KP = 30\n",
    "        \n",
    "    TRAIN_TT_LS = 4\n",
    "    SM_TT_LS = 4\n",
    "    CAR_TT_LS = 4\n",
    "    \n",
    "    TRAIN_TC_LS = 4\n",
    "    SM_TC_LS = 2\n",
    "    CAR_TC_LS = 4\n",
    "\n",
    "    TRAIN_HE_LS = 2   \n",
    "    SM_HE_LS = 2\n",
    "   \n",
    "    SM_SEATS_LS =2\n",
    "    GA_LS = 2\n",
    "    AGE_LS = 3\n",
    "    INCOME_LS = 2\n",
    "    PURPOSE_LS = 4\n",
    "\n",
    "    ACT_TR = 2\n",
    "    ACT_SM = 2\n",
    "    ACT_CAR = 2\n",
    "    learning_rate = 0.005 #0.0001 is very stable / 0.0005 is not BAD   \n",
    "\n",
    "    LatDNN = build_LatDNN(TRAIN_TT_KP,TRAIN_TT_LS,TRAIN_TC_KP,TRAIN_TC_LS,TRAIN_HE_KP,TRAIN_HE_LS,SM_TT_KP,SM_TT_LS,SM_TC_KP,SM_TC_LS,SM_HE_KP,SM_HE_LS,\n",
    "                     CAR_TT_KP,CAR_TT_LS,CAR_TC_KP,CAR_TC_LS,SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS,ACT_TR,ACT_SM,ACT_CAR)\n",
    "\n",
    "    batch_size = 128\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, verbose=1, patience=20)\n",
    "\n",
    "    history = LatDNN.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        shuffle=True,\n",
    "        epochs =200,\n",
    "        batch_size = batch_size,\n",
    "        validation_data = [x_valid,y_valid],\n",
    "        callbacks = [es],\n",
    "        verbose=1)\n",
    "\n",
    "    LatDNN.save_weights('C:/Users/euijin/Documents/LatDNN_Models/LatDNN_ASC_FF_SNU1_'+str(i))\n",
    "    #LatDNN.load_weights('C:/Users/euijin/Documents/LatDNN_Models/LatDNN_ASC_FF_SNU1_'+str(i))\n",
    "\n",
    "\n",
    "\n",
    "    # Get the prediction metrics\n",
    "    test_p = LatDNN.predict(x_test,verbose=0)\n",
    "    train_p = LatDNN.predict(x_train,verbose=0)\n",
    "\n",
    "    predict_train = np.argmax(train_p[0],axis=1)\n",
    "    predict_test =  np.argmax(test_p[0],axis=1)\n",
    "\n",
    "    y_train = np.array(df_train.CHOICE).reshape(-1,3)\n",
    "    y_test = np.array(df_test.CHOICE).reshape(-1,3)\n",
    "    y_train_cat = np.argmax(np.array(df_train.CHOICE).reshape(-1,3),axis=1)\n",
    "    y_test_cat = np.argmax(np.array(df_test.CHOICE).reshape(-1,3),axis=1)\n",
    "\n",
    "    test_NLL = round((F.cross_entropy(input=torch.log(tensor(test_p[0])),target=torch.Tensor(y_test)).numpy()).item(),3)\n",
    "    train_NLL =  round((F.cross_entropy(input=torch.log(tensor(train_p[0])),target=torch.Tensor(y_train)).numpy()).item(),3)\n",
    "\n",
    "    train_acc = round(accuracy_score(y_train_cat,predict_train),3)\n",
    "    test_acc = round(accuracy_score(y_test_cat,predict_test),3)\n",
    "\n",
    "    train_brier = round(brier_multi(y_train,train_p[0]),3)\n",
    "    test_brier = round(brier_multi(y_test,test_p[0]),3)\n",
    "\n",
    "    print([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a2aff-68b9-448b-8a66-db74599cab0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "## Repeated Experiments\n",
    "for i in range(50):\n",
    "\n",
    "    # record start time\n",
    "    start = time.time()\n",
    "\n",
    "    TRAIN_TT_KP = 10\n",
    "    SM_TT_KP = 30\n",
    "    CAR_TT_KP = 10\n",
    "\n",
    "    TRAIN_TC_KP = 20 # SNU2\n",
    "    SM_TC_KP = 10 # SNU2\n",
    "    CAR_TC_KP = 10\n",
    "    TRAIN_HE_KP = 10\n",
    "    SM_HE_KP = 30\n",
    "\n",
    "    TRAIN_TT_LS = 4\n",
    "    SM_TT_LS = 4\n",
    "    CAR_TT_LS = 4\n",
    "\n",
    "    TRAIN_TC_LS = 4 #\n",
    "    SM_TC_LS = 2\n",
    "    CAR_TC_LS = 4\n",
    "\n",
    "    TRAIN_HE_LS = 2   \n",
    "    SM_HE_LS = 2\n",
    "    SM_SEATS_LS =2\n",
    "    \n",
    "    GA_LS = 2\n",
    "    AGE_LS = 3\n",
    "    INCOME_LS = 2\n",
    "    PURPOSE_LS = 4\n",
    "\n",
    "    ACT_TR = 2\n",
    "    ACT_SM = 2\n",
    "    ACT_CAR = 2\n",
    "    learning_rate = 0.005 #0.0001 is very stable / 0.0005 is not BAD   \n",
    "    \n",
    "    LatDNN = build_LatDNN(TRAIN_TT_KP,TRAIN_TT_LS,TRAIN_TC_KP,TRAIN_TC_LS,TRAIN_HE_KP,TRAIN_HE_LS,SM_TT_KP,SM_TT_LS,SM_TC_KP,SM_TC_LS,SM_HE_KP,SM_HE_LS,\n",
    "                     CAR_TT_KP,CAR_TT_LS,CAR_TC_KP,CAR_TC_LS,SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS,ACT_TR,ACT_SM,ACT_CAR)\n",
    "\n",
    "    batch_size = 128\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, verbose=1, patience=20)\n",
    "\n",
    "    history = LatDNN.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        shuffle=True,\n",
    "        epochs =200,\n",
    "        batch_size = batch_size,\n",
    "        validation_data = [x_valid,y_valid],\n",
    "        callbacks = [es],\n",
    "        verbose=0)\n",
    "    \n",
    "\n",
    "    LatDNN.save_weights('C:/Users/euijin/Documents/LatDNN_Models_Revision/LatDNN_ASC_FF_'+str(i))\n",
    "    #LatDNN.load_weights('C:/Users/euijin/Documents/LatDNN_Models/LatDNN_ASC_FF_SNU2_'+str(i))\n",
    "    \n",
    "    # record end time\n",
    "    end = time.time()\n",
    "    training_time = (end-start)\n",
    "\n",
    "\n",
    "    # Get the prediction metrics\n",
    "    test_p = LatDNN.predict(x_test,verbose=0)\n",
    "    train_p = LatDNN.predict(x_train,verbose=0)\n",
    "\n",
    "    predict_train = np.argmax(train_p[0],axis=1)\n",
    "    predict_test =  np.argmax(test_p[0],axis=1)\n",
    "\n",
    "    y_train = np.array(df_train.CHOICE).reshape(-1,3)\n",
    "    y_test = np.array(df_test.CHOICE).reshape(-1,3)\n",
    "    y_train_cat = np.argmax(np.array(df_train.CHOICE).reshape(-1,3),axis=1)\n",
    "    y_test_cat = np.argmax(np.array(df_test.CHOICE).reshape(-1,3),axis=1)\n",
    "\n",
    "    test_NLL = round((F.cross_entropy(input=torch.log(tensor(test_p[0])),target=torch.Tensor(y_test)).numpy()).item(),3)\n",
    "    train_NLL =  round((F.cross_entropy(input=torch.log(tensor(train_p[0])),target=torch.Tensor(y_train)).numpy()).item(),3)\n",
    "\n",
    "    train_acc = round(accuracy_score(y_train_cat,predict_train),3)\n",
    "    test_acc = round(accuracy_score(y_test_cat,predict_test),3)\n",
    "\n",
    "    train_brier = round(brier_multi(y_train,train_p[0]),3)\n",
    "    test_brier = round(brier_multi(y_test,test_p[0]),3)\n",
    "\n",
    "    print([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])\n",
    "\n",
    "    ## Plot the PD and ICE\n",
    "    from sklearn.inspection import partial_dependence\n",
    "    from sklearn.inspection import PartialDependenceDisplay\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    class LatDNNFunction(BaseEstimator, ClassifierMixin):\n",
    "        def fit(self, X, y):\n",
    "            self.classes_ = unique_labels(y)\n",
    "            return self\n",
    "\n",
    "        def predict_proba(self, X):\n",
    "            self.proba_,_,_,_ = LatDNN.predict(X,verbose=0)\n",
    "            return np.array(self.proba_)\n",
    "\n",
    "        def decision_function(self, X):\n",
    "            self.utility_ = np.array(LatDNN.predict(X)[1:4]).transpose().reshape(-1,3)\n",
    "            return self.utility_\n",
    "\n",
    "    LatDNN_ALL = LatDNNFunction()\n",
    "    LatDNN_ALL.fit(x_train,y_train)\n",
    "\n",
    "    common_params = {\n",
    "        \"subsample\": 0.99999,\n",
    "        \"n_jobs\": 1,\n",
    "        \"grid_resolution\": 20,\n",
    "        \"centered\": True,\n",
    "        \"random_state\": 1,\n",
    "        \"response_method\": 'decision_function',\n",
    "        \"percentiles\": (0.01, 0.99),\n",
    "\n",
    "    }\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\n",
    "\n",
    "    display_0 = PartialDependenceDisplay.from_estimator(\n",
    "        LatDNN_ALL,\n",
    "        x_train,\n",
    "        features=['TRAIN_TT','TRAIN_TC'], # TC,TT\n",
    "        kind=[\"both\",\"both\"],\n",
    "        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\n",
    "        ax=ax,\n",
    "        target = 0,\n",
    "        **common_params,\n",
    "    )\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\n",
    "\n",
    "    display_1 = PartialDependenceDisplay.from_estimator(\n",
    "        LatDNN_ALL,\n",
    "        x_train,\n",
    "        features=['SM_TT','SM_TC'], # TC,TT\n",
    "        kind=[\"both\",\"both\"],\n",
    "        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\n",
    "        ax=ax,\n",
    "        target = 1,\n",
    "        **common_params,\n",
    "    )\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\n",
    "    \n",
    "    display_2 = PartialDependenceDisplay.from_estimator(\n",
    "        LatDNN_ALL,\n",
    "        x_train,\n",
    "        features=['CAR_TT','CAR_TC'], # TC,TT\n",
    "        kind=[\"both\",\"both\"],\n",
    "        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\n",
    "        ax=ax,\n",
    "        target = 2,\n",
    "        **common_params,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    ## Population-level Parameters\n",
    "\n",
    "    PD_TCNN_TT_TR = display_0.pd_results[0]['average'][0]\n",
    "    PD_TCNN_TC_TR = display_0.pd_results[1]['average'][0] \n",
    "\n",
    "    PD_TCNN_TT_SM = display_1.pd_results[0]['average'][1]\n",
    "    PD_TCNN_TC_SM = display_1.pd_results[1]['average'][1] \n",
    "\n",
    "    PD_TCNN_TT_CAR = display_2.pd_results[0]['average'][2]\n",
    "    PD_TCNN_TC_CAR = display_2.pd_results[1]['average'][2] \n",
    "\n",
    "    PD_values_TT_TR = display_0.pd_results[0]['values'][0]\n",
    "    PD_values_TC_TR = display_0.pd_results[1]['values'][0]\n",
    "    \n",
    "    PD_values_TT_SM = display_1.pd_results[0]['values'][0]\n",
    "    PD_values_TC_SM = display_1.pd_results[1]['values'][0]\n",
    "    \n",
    "    PD_values_TT_CAR = display_2.pd_results[0]['values'][0]\n",
    "    PD_values_TC_CAR = display_2.pd_results[1]['values'][0]\n",
    "\n",
    "    beta_TCNN_TT_TR = (np.diff(PD_TCNN_TT_TR)/np.diff(PD_values_TT_TR))\n",
    "    beta_TCNN_TC_TR = (np.diff(PD_TCNN_TC_TR)/np.diff(PD_values_TC_TR))\n",
    "\n",
    "    beta_TCNN_TT_SM = (np.diff(PD_TCNN_TT_SM)/np.diff(PD_values_TT_SM))\n",
    "    beta_TCNN_TC_SM = (np.diff(PD_TCNN_TC_SM)/np.diff(PD_values_TC_SM))\n",
    "\n",
    "    beta_TCNN_TT_CAR = (np.diff(PD_TCNN_TT_CAR)/np.diff(PD_values_TT_CAR))\n",
    "    beta_TCNN_TC_CAR = (np.diff(PD_TCNN_TC_CAR)/np.diff(PD_values_TC_CAR))\n",
    "\n",
    "\n",
    "    TC_TR_IDX =(beta_TCNN_TC_TR != 0)\n",
    "    TC_SM_IDX =(beta_TCNN_TC_SM != 0)\n",
    "    TC_CAR_IDX =(beta_TCNN_TC_CAR != 0)\n",
    "\n",
    "    beta_TCNN_TC_TR = beta_TCNN_TC_TR[TC_TR_IDX]\n",
    "    beta_TCNN_TC_SM = beta_TCNN_TC_SM[TC_SM_IDX]\n",
    "    beta_TCNN_TC_CAR = beta_TCNN_TC_CAR[TC_CAR_IDX]\n",
    "    \n",
    "    beta_TCNN_TT_TR = beta_TCNN_TT_TR[TC_TR_IDX]\n",
    "    beta_TCNN_TT_SM = beta_TCNN_TT_SM[TC_SM_IDX]\n",
    "    beta_TCNN_TT_CAR = beta_TCNN_TT_CAR[TC_CAR_IDX]\n",
    "\n",
    "    \n",
    "    beta_TCNN_VOT_POP_TR = np.median(compute_VOT_POP(beta_TCNN_TT_TR,beta_TCNN_TC_TR))\n",
    "    beta_TCNN_VOT_POP_SM = np.median(compute_VOT_POP(beta_TCNN_TT_SM,beta_TCNN_TC_SM))\n",
    "    beta_TCNN_VOT_POP_CAR = np.median(compute_VOT_POP(beta_TCNN_TT_CAR,beta_TCNN_TC_CAR))\n",
    "\n",
    "\n",
    "    ## Individual-level Parameters\n",
    "\n",
    "    ICE_TCNN_TT_TR = display_0.pd_results[0]['individual'][0]\n",
    "    ICE_TCNN_TC_TR = display_0.pd_results[1]['individual'][0]\n",
    "\n",
    "    ICE_TCNN_TT_SM = display_1.pd_results[0]['individual'][1]\n",
    "    ICE_TCNN_TC_SM = display_1.pd_results[1]['individual'][1]\n",
    "\n",
    "    ICE_TCNN_TT_CAR = display_2.pd_results[0]['individual'][2]\n",
    "    ICE_TCNN_TC_CAR = display_2.pd_results[1]['individual'][2]\n",
    "\n",
    "    beta_TCNN_TT_TR_ICE = (np.diff(ICE_TCNN_TT_TR)/np.diff(PD_values_TT_TR))\n",
    "    beta_TCNN_TC_TR_ICE = (np.diff(ICE_TCNN_TC_TR)/np.diff(PD_values_TC_TR))\n",
    "    beta_TCNN_VOT_TR_ICE = compute_VOT_IND(beta_TCNN_TT_TR_ICE[:,TC_TR_IDX],beta_TCNN_TC_TR_ICE[:,TC_TR_IDX])\n",
    "\n",
    "    beta_TCNN_TT_SM_ICE = (np.diff(ICE_TCNN_TT_SM)/np.diff(PD_values_TT_SM))\n",
    "    beta_TCNN_TC_SM_ICE = (np.diff(ICE_TCNN_TC_SM)/np.diff(PD_values_TC_SM))\n",
    "    beta_TCNN_VOT_SM_ICE = compute_VOT_IND(beta_TCNN_TT_SM_ICE[:,TC_SM_IDX],beta_TCNN_TC_SM_ICE[:,TC_SM_IDX])\n",
    "\n",
    "\n",
    "    beta_TCNN_TT_CAR_ICE = (np.diff(ICE_TCNN_TT_CAR)/np.diff(PD_values_TT_CAR))\n",
    "    beta_TCNN_TC_CAR_ICE = (np.diff(ICE_TCNN_TC_CAR)/np.diff(PD_values_TC_CAR))\n",
    "    beta_TCNN_VOT_CAR_ICE = compute_VOT_IND(beta_TCNN_TT_CAR_ICE[:,TC_CAR_IDX],beta_TCNN_TC_CAR_ICE[:,TC_CAR_IDX])\n",
    "\n",
    "\n",
    "    # Estimation # If we use the median value, we don't need to care about the outlier or somethings.\n",
    "  \n",
    "    x_test_E = x_train[['AGES','INCOMES','PURPOSES']].reset_index(drop=True)\n",
    "    x_test_E['VOT_TR'] = beta_TCNN_VOT_TR_ICE\n",
    "    x_test_E['VOT_SM'] = beta_TCNN_VOT_SM_ICE\n",
    "    x_test_E['VOT_CAR'] = beta_TCNN_VOT_CAR_ICE\n",
    "\n",
    "    VOT_TCNN_IND = x_test_E[['AGES','INCOMES','PURPOSES','VOT_TR','VOT_SM','VOT_CAR']].groupby(['AGES','INCOMES','PURPOSES']).median()\n",
    "   \n",
    "        \n",
    "    print(np.quantile(VOT_TCNN_IND['VOT_TR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\n",
    "    print(np.quantile(VOT_TCNN_IND['VOT_SM'],(0.5,0.01,0.25,0.75,0.99)).round(3))\n",
    "    print(np.quantile(VOT_TCNN_IND['VOT_CAR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\n",
    "    \n",
    "\n",
    "    # Estimation results\n",
    "    PRED_PERF = np.array([train_acc,test_acc,train_brier,test_brier]).transpose()\n",
    "    EST_RESULTS = [PRED_PERF,VOT_TCNN_IND,training_time]\n",
    "    \n",
    "    \n",
    "    with open(\"C:/Users/euijin/Documents/LatDNN_Results_Revision/ASC_RESULTS_FF_\"+str(i), \"wb\") as f:\n",
    "        pickle.dump(EST_RESULTS, f)\n",
    "    #np.savetxt(\"C:/Users/euijin/Documents/LatDNN_Results_Revision/ASC_RESULTS_FF_\"+str(i),EST_RESULTS)\n",
    "    #save_clipboard(np.concatenate([PRED_PERF,VOT_TCNN_IND.median(axis=0).round(3)])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e62a38-9c96-40f9-8999-0b51adfe4ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "## Results analysis\n",
    "PRED_PERF_S = []\n",
    "VOT_TCNN_IND_S = []\n",
    "VOT_TCNN_index = []\n",
    "\n",
    "for i in range(18):\n",
    "    with open(\"C:/Users/euijin/Documents/LatDNN_Results_Revision/ASC_RESULTS_FF_\"+str(i),'rb') as f:\n",
    "        temp = pickle.load(f)  \n",
    "    PRED_PERF_S.append(temp[0])\n",
    "    VOT_TCNN_IND_S.append(temp[1])\n",
    "    VOT_TCNN_index.append(temp[1].index.values)\n",
    "    \n",
    "\n",
    "## Predictability    \n",
    "PRED_PERF_S = np.array(PRED_PERF_S)\n",
    "A = np.concatenate([PRED_PERF_S.mean(axis=0).round(3).reshape(-1,1),PRED_PERF_S.std(axis=0).round(3).reshape(-1,1)],axis=1)\n",
    "\n",
    "\n",
    "## Interpretability\n",
    "IND_stats = []\n",
    "VOT_TCNN_IND_S_TR = []\n",
    "VOT_TCNN_IND_S_SM = []\n",
    "VOT_TCNN_IND_S_CAR = []\n",
    "\n",
    "for i in range(len(VOT_TCNN_IND_S)):\n",
    "    \n",
    "    inter_IDX_IND = VOT_TCNN_IND_S[i].index.isin(np.intersect1d(VOT_TCNN_IND_S[i].index,df_VOT_B.index))\n",
    "        \n",
    "    TRIND = np.quantile(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,0],(0.5,0.01,0.25,0.75,0.99)).round(3)\n",
    "    SMIND = np.quantile(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,1],(0.5,0.01,0.25,0.75,0.99)).round(3)\n",
    "    CARIND = np.quantile(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,2],(0.5,0.01,0.25,0.75,0.99)).round(3)   \n",
    "    ALLIND = np.concatenate([TRIND,SMIND,CARIND])   \n",
    "    IND_stats.append(ALLIND)\n",
    "    \n",
    "    VOT_TCNN_IND_S_TR.append(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,0])\n",
    "    VOT_TCNN_IND_S_SM.append(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,1])\n",
    "    VOT_TCNN_IND_S_CAR.append(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,2])\n",
    "      \n",
    "B = np.array(IND_stats)\n",
    "B = np.concatenate([np.median(B,axis=0).round(3).reshape(-1,1),\n",
    "                    stats.iqr(B,axis=0).round(3).reshape(-1,1)],axis=1)\n",
    "\n",
    "\n",
    "VOT_TCNN_IND_S_TR = np.concatenate(VOT_TCNN_IND_S_TR)\n",
    "VOT_TCNN_IND_S_SM = np.concatenate(VOT_TCNN_IND_S_SM)\n",
    "VOT_TCNN_IND_S_CAR = np.concatenate(VOT_TCNN_IND_S_CAR)\n",
    "\n",
    "EST_RESULTS_TCNN = np.vstack([B,A])\n",
    "save_clipboard(EST_RESULTS_TCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1808265-4523-4543-9ef0-388081483023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "## Results analysis of MNL\n",
    "\n",
    "PRED_PERF_S = []\n",
    "PARAM_PERF_S = []\n",
    "VOT_MNL_IND_S = []\n",
    "VOT_TRUE_S = []\n",
    "VOT_MNL_index = []\n",
    "\n",
    "\n",
    "## Results analysis of TCNN\n",
    "\n",
    "PRED_PERF_S = []\n",
    "PARAM_PERF_S = []\n",
    "VOT_TCNN_IND_S = []\n",
    "VOT_TRUE_S = []\n",
    "VOT_TCNN_index = []\n",
    "\n",
    "for i in range(1):\n",
    "    with open(\"C:/Users/euijin/Documents/LatDNN_Results_Revision/ASC_RESULTS_FF_\"+str(i),'rb') as f:\n",
    "        temp = pickle.load(f)  \n",
    "    PRED_PERF_S.append(temp[0])\n",
    "    VOT_TCNN_IND_S.append(temp[1])\n",
    "    VOT_TCNN_index.append(temp[1].index.values)\n",
    "\n",
    "df_VOT_TCNN = pd.DataFrame(np.vstack(VOT_TCNN_IND_S),index = np.hstack(VOT_TCNN_index),columns = ['VOT_TR','VOT_SM','VOT_CAR'])\n",
    "\n",
    "\n",
    "## Results analysis of DNN\n",
    "PRED_PERF_S = []\n",
    "PARAM_PERF_S = []\n",
    "VOT_DNN_IND_S = []\n",
    "VOT_TRUE_S = []\n",
    "VOT_DNN_index = []\n",
    "\n",
    "for i in range(1):\n",
    "    with open(\"C:/Users/euijin/Documents/DNN_Results_Revision/FUL_RESULTS_FF_\"+str(i),'rb') as f:\n",
    "        temp = pickle.load(f)  \n",
    "    PRED_PERF_S.append(temp[0])\n",
    "    VOT_DNN_IND_S.append(temp[1])\n",
    "    VOT_DNN_index.append(temp[1].index.values)\n",
    "\n",
    "df_VOT_DNN = pd.DataFrame(np.vstack(VOT_DNN_IND_S),index = np.hstack(VOT_DNN_index),columns = ['VOT_TR','VOT_SM','VOT_CAR'])\n",
    "\n",
    "## Results analysis of MNL\n",
    "df_VOT_MNL = pd.DataFrame(df_VOT_B.values[:,1:4],index = df_VOT_B.index.values,columns =['VOT_TR','VOT_SM','VOT_CAR'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
